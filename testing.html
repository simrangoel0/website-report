<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Blueprint AI - Testing</title>
  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
  <!-- Your Custom CSS -->
  <link rel="stylesheet" href="styles.css">
  <!-- Font Awesome (if needed) -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
</head>
<body class="d-flex flex-column min-vh-100">

  <header>
    <!-- Navigation Bar -->
    <nav class="navbar navbar-expand-lg">
      <div class="container">
        <!-- Logo/Brand -->
        <a class="navbar-brand" href="index.html">
          <img src="logoText.png" alt="Blueprint AI Logo" class="navbar-logo">
        </a>
        <!-- Toggle for mobile view -->
        <button
          class="navbar-toggler"
          type="button"
          data-bs-toggle="collapse"
          data-bs-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>

        <!-- Nav links -->
        <div class="collapse navbar-collapse" id="navbarNav">
          <ul class="navbar-nav ms-auto">
            <!-- Home Link -->
            <li class="nav-item">
              <a class="nav-link" href="index.html">Home</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="requirements.html">Requirements</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="research.html">Research</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="ui-design.html">UI Design</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="system-design.html">System Design</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="implementation.html">Implementation</a>
            </li>
            <!-- Testing link marked as active with a dropdown -->
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle active"
                 href="testing.html"
                 id="testingDropdown"
                 role="button"
                 data-bs-toggle="dropdown"
                 aria-expanded="false">
                Testing
              </a>
              <ul class="dropdown-menu" aria-labelledby="testingDropdown">
                <li><a class="dropdown-item" href="testing.html#testing-strategy">Testing Strategy</a></li>
                <li><a class="dropdown-item" href="testing.html#unit-and-integration-testing">Unit and integration testing</a></li>
                <li><a class="dropdown-item" href="testing.html#compatibility-testing">Compatibility testing</a></li>
                <li><a class="dropdown-item" href="testing.html#responsive-design-testing">Responsive design testing</a></li>
                <li><a class="dropdown-item" href="testing.html#performance-stress-testing">Performance/stress testing</a></li>
                <li><a class="dropdown-item" href="testing.html#user-acceptance-testing">User acceptance testing</a></li>
              </ul>
            </li>

            <!-- Remaining nav links -->
            <li class="nav-item">
              <a class="nav-link" href="evaluation.html">Evaluation</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="appendices.html">Appendices</a>
            </li>    
            <li class="nav-item">
                <a class="nav-link" href="blog.html">Blog</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

<!-- Page Title/Intro -->
<h1 class="my-5" style="font-size:2rem; margin-bottom:1rem;">Testing</h1>
<p style="margin-bottom:2rem;">
  How we validated system reliability, compatibility, and performance under
  various conditions, ensuring quality and user satisfaction.
</p>
</header>

<!-- Main Content -->
<main class="container flex-grow-1">

  <!-- Testing Strategy -->
  <section id="testing-strategy" class="mb-5" style="margin-bottom:2rem;">
    <h2 style="font-size:1.75rem; margin-bottom:1rem;">Testing Strategy</h2>
    <p style="margin-bottom:1rem;">
      Our testing strategy embodies a comprehensive, end-to-end approach that covers
      every aspect of the project’s AI-driven web page creation pipeline, user interface
      rendering, and underlying Python OCR service. The primary goal is to validate not
      only individual features—such as text extraction from screenshots, AI summarization
      for UI structure, and final layout generation—but also the broader integration
      points that unite these components into a cohesive user experience.
    </p>
    <p style="margin-bottom:1rem;">
      By adopting a layered methodology, we ensure high accuracy and reliability when
      creating fully functional web pages from textual descriptions and screenshot
      references. Each stage of the AI flow is systematically examined, along with key
      user interface interactions in the CraftJS editor and the performance of the Python
      services powering OCR and data transfer. This approach guarantees that users
      experience a smooth, consistent workflow, from providing a prompt or image to
      receiving a fully rendered, interactive layout.
    </p>
    <h3 style="font-size:1.25rem; margin-bottom:0.5rem;">Testing Scope</h3>
    <p style="margin-bottom:1rem;">
      The testing scope encompasses all modules and services within the project, ensuring
      that each segment of the AI flow and user interface is rigorously examined for
      correctness, reliability, and maintainability. This includes:
    </p>
    <ul style="margin-bottom:1rem;">
      <p>- <strong>Python OCR (EasyOCR) in the python-ocr folder:</strong> Validates text extraction accuracy and performance.</p> 
      <p>- <strong>AI Summaries and Final Layout Generation within the ai directory:</strong> Ensures prompt handling, summarization accuracy, and JSON layout conformity with CraftJS requirements.</p> 
      <p>- <strong>Main Web Application and User Interface under webview-ui:</strong> Tests React components, sidebars, drag-and-drop CraftJS canvas, and AI chat flows.</p> 
      <p>- <strong>VS Code Extension Wrapper in src:</strong> Verifies robust communication between the extension and the front-end modules.</p> 
      <p>- <strong>Export and Packaging Features:</strong> Checks HTML/CSS/JS exports, code generation, and user workflows to move finished designs into real-world environments.</p> 
    </ul>
    <h3 style="font-size:1.25rem; margin-bottom:0.5rem;">Automated and Manual Testing</h3>
    <p style="margin-bottom:1rem;">
      A structured mix of automated and manual tests ensures that every critical path is
      covered while also enabling exploratory testing. Automated tests provide repeatable,
      fast feedback on core functionalities such as Python OCR, AI prompt handling, and UI
      components. Manual tests focus on usability, design fidelity, and user acceptance by
      simulating real-world usage. This dual testing approach maintains a robust safety net
      that yields high confidence in each release.
    </p>
    <h3 style="font-size:1.25rem; margin-bottom:0.5rem;">Compatibility and Hardware Testing</h3>
    <p style="margin-bottom:1rem;">
      Compatibility and hardware testing ensures that the entire AI-driven page generation
      workflow operates seamlessly on Windows, macOS, and Linux platforms, across diverse
      hardware configurations. We confirm Python OCR performance and accuracy on machines
      with varying CPU and GPU capabilities by running repeated extractions on screenshots
      of different resolutions and file sizes.
    </p>
    <p style="margin-bottom:1rem;">
      Multiple screen sizes and resolutions are also tested to guarantee correct
      responsiveness. Peripherals such as touchscreens are validated to confirm drag-and-drop
      mechanics in the CraftJS canvas. By thoroughly verifying cross-platform stability,
      the project meets user expectations under all operating conditions.
    </p>
  </section>

 <!-- Unit and Integration Testing -->
<section id="unit-and-integration-testing" class="mb-5" style="margin-bottom:2rem;">
  <h2 style="font-size:1.75rem; margin-bottom:1rem;">Unit and Integration Testing</h2>

  <p style="margin-bottom:1rem;">
    We employ a comprehensive blend of Python <em>unittest</em> and JavaScript/TypeScript
    testing frameworks to verify both modular correctness and system-wide integrity. This
    dual-layered approach ensures that each core functionality—ranging from OCR text
    extraction to the AI-driven summarization—operates reliably on its own and in tandem
    with the rest of the project’s components.
  </p>

  <h3 style="font-size:1.25rem; margin-bottom:0.5rem;">Python OCR Unit Tests</h3>
<p style="margin-bottom:1rem;">
  For OCR-related functionality, we rely on an extensive and carefully designed test suite 
  built with Python’s <strong>unittest</strong> framework. Each test case within this suite 
  is meant to precisely reflect potential real-world conditions that our OCR pipeline 
  might encounter when converting screenshots into workable text for our AI summarization 
  workflow. By structuring tests around specific, well-defined scenarios, we gain deeper 
  insights into how each step of the process handles both typical and edge-case conditions.
</p>
<p style="margin-bottom:1rem;">
  At a high level, our Python OCR unit tests validate five fundamental aspects of the process:
</p>
<ol style="margin-bottom:1rem; list-style-type: decimal; margin-left:2rem;">
  <p>- <strong>Preprocessing Efficacy:</strong> We check whether applying grayscale, upscaling, 
    or noise reduction steps before running EasyOCR significantly boosts text recognition 
    results. Test images range from crisp, high-resolution screenshots to heavily compressed 
    or low-contrast images, ensuring that preprocessing steps consistently enhance 
    accuracy rather than degrade it.</p> 

  <p>- <strong>Bounding Box Calculation:</strong> Each recognized text element returns a set 
    of coordinates representing the bounding box. We validate these against an acceptable 
    size range defined in our test cases. For instance, a small label on a form might require 
    a bounding box area of 40–100 pixels, whereas a larger heading might span hundreds or 
    even thousands of pixels in area. By confirming these ranges, we ensure our system 
    accurately pinpoints textual regions.</p> 

  <p>- <strong>Confidence Thresholding:</strong> We measure the average and individual 
    confidence scores from the OCR results to confirm that they meet or exceed minimum 
    thresholds. For example, certain test images mandate a confidence score of at least 0.80 
    to pass. This assures us that the recognized text is reliable enough to feed into the AI 
    summarization steps without introducing spurious or low-accuracy content.</p> 

  <p>- <strong>Substring Verification:</strong> Each test case includes an <code>expected_substring</code> 
    value. By scanning the full recognized text for that substring, we verify that crucial 
    details—like an invoice number, a billing label, or a receipt total—appear exactly as 
    anticipated. For real-world usage, missing or incorrect substrings might lead to user 
    confusion or broken AI analysis (e.g., a chat prompt about an invoice failing because 
    the word "Invoice" never got recognized).</p> 

  <p>- <strong>JSON Output Consistency:</strong> Once we generate final OCR data, the system 
    encodes it as a JSON array of recognized text items, each with <code>text</code>, 
    <code>confidence</code>, and <code>bbox</code> fields. Our tests parse this JSON to ensure 
    it’s valid and that each data entry includes all required fields. This consistency is 
    critical for the subsequent steps in the pipeline, where the AI summarization relies on 
    structured JSON rather than raw, unformatted text.</p> 
</ol>

<p style="margin-bottom:1rem;">
  Our test images are carefully curated to cover diverse scenarios: <strong>Receipts</strong> 
  often have small, dense text blocks and varied fonts, <strong>Billing Forms</strong> can 
  include vertical or horizontally aligned text with an array of labels, and 
  <strong>Invoices</strong> typically feature tables, prominent headings, and sections such 
  as “Invoice #” or “Amount Due.” By systematically running these tests and evaluating the 
  results, we confirm that the OCR handles each situation reliably, alerting us to any 
  regressions or degradations in accuracy as our codebase evolves.
</p>
<p style="margin-bottom:1rem;">
  From an architectural perspective, the Python tests also double as a form of contract 
  testing for the AI module that follows. By ensuring the detected text is both accurate 
  and well-structured, the subsequent AI requests (e.g., for UI summarization) can assume 
  that they’re getting clean, precise information. This helps minimize the probability of 
  cascading errors, where flawed OCR output might cause the AI to misunderstand essential 
  labels (like “Username” or “Submit Button”) within the CraftJS-generated layout. 
</p>
<p style="margin-bottom:1rem;">
  Finally, these Python OCR unit tests are integrated into our continuous integration (CI) 
  workflow. Whenever a developer pushes a change, the entire OCR test suite is automatically 
  triggered, providing immediate feedback if any adjustment—say, introducing a new 
  preprocessing technique—negatively impacts performance or accuracy. This proactive testing 
  stance, combined with the depth of our scenario coverage, ensures that our OCR functionality 
  remains robust and continues to support the broader AI-driven architecture over time.
</p>


<pre style="background-color:#f8f8f8; padding:1rem; overflow:auto; text-align:left; max-width: 80%; width: 80%; margin: 0 auto;">
  <code>
import unittest
import os
import json
from typing import List, Dict, Any
from ocr_service import minimal_preprocess, run_easyocr

class TestOCRService(unittest.TestCase):
    """
    This class checks recognized text, bounding boxes, and confidence values.
    """

    def setUp(self):
        """
        Establishes a list of test cases with known images and corresponding expectations.
        Each element contains:
          - img_path: Path to a local image.
          - expected_substring: A phrase expected in the recognized text.
          - approximate_confidence: A lower threshold to validate confidence.
          - bounding_box_range: Approximate bounding box size to confirm detection coords.
        """
        self.test_cases: List[Dict[str, Any]] = [
            {
                "img_path": "tests/data/receipt_sample.png",
                "expected_substring": "Total: 15.99",
                "approximate_confidence": 0.80,
                "bounding_box_range": (50, 1000)
            },
            {
                "img_path": "tests/data/invoice_sample.png",
                "expected_substring": "Invoice #1234",
                "approximate_confidence": 0.75,
                "bounding_box_range": (80, 1200)
            },
            {
                "img_path": "tests/data/billing_form.jpg",
                "expected_substring": "Billing Information",
                "approximate_confidence": 0.70,
                "bounding_box_range": (60, 1500)
            }
        ]

    def test_text_extraction(self):
        """
        Validates that the recognized text output contains the expected substring
        for each sample image. The output text is aggregated into a single string
        for a substring check.
        """
        for case in self.test_cases:
            processed_image = minimal_preprocess(case["img_path"])
            ocr_results = run_easyocr(processed_image)
            recognized_text = " ".join([item[1] for item in ocr_results])
            self.assertIn(case["expected_substring"], recognized_text)

    def test_confidence_values(self):
        """
        Verifies that recognized text elements maintain an average confidence
        above an approximate threshold, using final outputs from run_easyocr.
        """
        for case in self.test_cases:
            processed_image = minimal_preprocess(case["img_path"])
            ocr_results = run_easyocr(processed_image)
            confidences = []
            for item in ocr_results:
                if len(item) >= 3:
                    confidences.append(float(item[2]))
            avg_conf = sum(confidences) / max(len(confidences), 1)
            self.assertGreaterEqual(
                avg_conf,
                case["approximate_confidence"],
                msg=f"Average confidence is {avg_conf}, lower than expected."
            )

    def test_bounding_boxes(self):
        """
        Confirms that bounding boxes for recognized text fall within a
        reasonable range, ensuring coordinate extraction is consistent.
        """
        for case in self.test_cases:
            processed_image = minimal_preprocess(case["img_path"])
            ocr_results = run_easyocr(processed_image)
            min_box_size, max_box_size = case["bounding_box_range"]
            for item in ocr_results:
                coords = item[0]
                xs = [pt[0] for pt in coords]
                ys = [pt[1] for pt in coords]
                width = max(xs) - min(xs)
                height = max(ys) - min(ys)
                box_area = width * height
                self.assertGreaterEqual(
                    box_area,
                    min_box_size,
                    msg=f"Box area of {box_area} is below {min_box_size}."
                )
                self.assertLessEqual(
                    box_area,
                    max_box_size,
                    msg=f"Box area of {box_area} is above {max_box_size}."
                )

    def test_output_structure(self):
        """
        Checks that the final JSON structure is well-formed and includes
        'text', 'confidence', and 'bbox' fields for each recognized item.
        """
        for case in self.test_cases:
            processed_image = minimal_preprocess(case["img_path"])
            results = run_easyocr(processed_image)
            output_data = []
            for result in results:
                coords = result[0]
                text_val = result[1]
                conf_val = result[2] if len(result) >= 3 else 1.0
                xs = [pt[0] for pt in coords]
                ys = [pt[1] for pt in coords]
                min_x, max_x = int(min(xs)), int(max(xs))
                min_y, max_y = int(min(ys)), int(max(ys))
                data_entry = {
                    "text": text_val,
                    "confidence": float(conf_val),
                    "bbox": [min_x, min_y, max_x, max_y]
                }
                output_data.append(data_entry)

            json_str = json.dumps(output_data, ensure_ascii=False)
            parsed = json.loads(json_str)
            self.assertIsInstance(parsed, list)
            for item in parsed:
                self.assertIn("text", item)
                self.assertIn("confidence", item)
                self.assertIn("bbox", item)
                self.assertIsInstance(item["bbox"], list)

if __name__ == '__main__':
    unittest.main()
</code>
  </pre>

  <h3 style="font-size:1.25rem; margin-bottom:0.5rem;">Integration with AI Summaries</h3>
  <p style="margin-bottom:1rem;">
    Our Python OCR tests serve as the first critical step in an integrated pipeline that
    continues with AI-driven summarization. Whenever a screenshot is processed, each recognized
    text fragment—along with its corresponding confidence score and bounding box—feeds directly
    into our AI summarization routines. This sequence is particularly important when the AI is
    attempting to group and interpret distinct UI elements, such as buttons, labels, or form
    fields.
  </p>
  <p style="margin-bottom:1rem;">
    If the OCR data is inaccurate or misaligned, the summarization model may incorrectly classify
    certain parts of the interface, leading to poorly structured CraftJS layouts. For instance,
    an incorrectly recognized label (e.g., “Submiit” instead of “Submit”) might prompt the AI to
    overlook important functions or produce an illogical element hierarchy. Similarly, a mismatch
    in bounding boxes can cause the AI to merge distinct text blocks into a single item, producing
    misleading or broken interface components. Therefore, accurate, well-tested OCR output 
    substantially reduces error propagation and ensures our summarization steps yield meaningful
    UI/GUI outlines.
  </p>
  <p style="margin-bottom:1rem;">
    We also validate the interplay between OCR accuracy and AI text grouping. Common real-world
    scenarios include receipts with itemized lists, form pages with overlapping fields, or
    dense landing pages featuring multiple navigation bars. By testing OCR performance on these
    complex visuals and then measuring how accurately the AI organizes them (into nav bars,
    headers, or content blocks), we confirm that the entire end-to-end pipeline remains robust.
    Our Python unit tests, in turn, give us confidence that any anomalies in AI output stem from
    legitimate modeling complexities rather than faulty input data.
  </p>
  
  <h3 style="font-size:1.25rem; margin-bottom:0.5rem;">TypeScript Integration and Performance Tests</h3>
  <p style="margin-bottom:1rem;">
    On the JavaScript/TypeScript side, we perform comprehensive integration checks that monitor
    the data flow from OCR (Python) all the way to the AI summarization logic and final layout
    rendering. These tests verify that large or partially accurate recognized text is still
    handled gracefully by the AI. For instance, even if some tokens are unrecognized or
    truncated, our summarization prompts and fallback logic should interpret the remaining text
    in a robust manner, maintaining essential structure and labeling within the resulting CraftJS
    blueprint.
  </p>
  <p style="margin-bottom:1rem;">
    We also account for concurrency and real-time feedback. Multiple AI requests (e.g., repeated
    summarizations after small user edits) must not degrade system performance to an unacceptable
    level. The TypeScript integration tests simulate situations where the user uploads multiple
    screenshots of varying sizes—like a straightforward login form followed by a large, image-heavy
    landing page—in quick succession. We measure processing speeds and monitor memory usage to
    ensure our solution scales without crashing or causing severe slowdowns. Concurrency checks
    also focus on verifying that partial outputs from the OCR service do not lock the AI summarization
    routines.
  </p>
  <p style="margin-bottom:1rem;">
    Regarding performance, we maintain detailed logging of every stage: from the start of OCR 
    processing to the final generation of a CraftJS layout. This data is aggregated in test
    reports, allowing us to pinpoint bottlenecks or regressions when the codebase evolves. For
    example, if the OCR step suddenly spikes from 20 seconds to 60 seconds for a moderately
    complex screenshot, we receive an immediate alert through our continuous integration pipeline.
    We can then trace that spike to a specific commit or configuration change (e.g., a newly
    introduced preprocessing step) and address it before it affects user-facing builds.
  </p>
  <p style="margin-bottom:1rem;">
    The TypeScript tests also perform content validation on AI outputs beyond mere timing. We
    inspect the textual summaries (both UI and GUI) to confirm they include vital elements,
    such as “Submit Button” or “Navigation Bar,” and do not produce contradictory or impossible
    layout structures. By combining these content checks with timing metrics, our integration
    suite ensures both correctness and efficiency in real-world usage scenarios. The following
    snippet (<strong>aiOcrPerformance.integration.test.ts</strong>) demonstrates how we track
    and log these performance benchmarks, making them an essential part of regression detection
    and capacity planning.
  </p>
  

  <pre style="background-color:#f8f8f8; padding:1rem; overflow:auto; text-align:left; max-width: 80%; width: 80%; margin: 0 auto;">
    <code>
/**
 * @file aiOcrPerformance.integration.test.ts
 *
 * Measures how long OCR and subsequent AI summarization take for images
 * of varying complexity in the Blueprint AI Project. 
 * It logs or asserts runtimes to track performance trends.
 */

import path from 'path';
import fs from 'fs';
import { performance } from 'perf_hooks';
import { runPythonOcr } from '../../src/ai/pythonBridge';
import { getUiSummary, getGuiSummary } from '../../src/ai/blueprintAiClient';

describe('AI + OCR Performance Integration Tests', () => {
  interface TestImageCase {
    name: string;
    filename: string;
    complexityLevel: string;
  }

  const testImages: TestImageCase[] = [
    { name: 'Very Simple', filename: 'very-simple.png', complexityLevel: 'very-simple' },
    { name: 'Simple', filename: 'simple.png', complexityLevel: 'simple' },
    { name: 'Moderate', filename: 'moderate.png', complexityLevel: 'moderate' },
    { name: 'Complex', filename: 'complex.png', complexityLevel: 'complex' },
    { name: 'Dense', filename: 'dense.png', complexityLevel: 'dense' },
    { name: 'Extremely Dense', filename: 'extremely-dense.png', complexityLevel: 'extremely-dense' },
  ];

  const FAKE_OPENAI_KEY = 'sk-xxxxxxx-FAKE-TEST-KEY';

  testImages.forEach((testCase) => {
    it(\`should measure OCR and AI time for image: \${testCase.name}\`, async () => {
      const imagePath = path.resolve(__dirname, '..', 'data', testCase.filename);

      if (!fs.existsSync(imagePath)) {
        throw new Error(\`Test image not found at path: \${imagePath}\`);
      }

      const rawImageBuffer = fs.readFileSync(imagePath);

      // Measure OCR time
      const ocrStart = performance.now();
      const ocrResult = await runPythonOcr(rawImageBuffer);
      const ocrEnd = performance.now();
      const ocrTimeMs = ocrEnd - ocrStart;

      // Build recognized text
      const recognizedText = ocrResult.map((item: any) => item.text).join('\\n');

      // Measure UI summary time
      const uiStart = performance.now();
      const uiSummary = await getUiSummary({
        text: recognizedText,
        screenshot: rawImageBuffer,
        openAiKey: FAKE_OPENAI_KEY,
      });
      const uiEnd = performance.now();
      const uiTimeMs = uiEnd - uiStart;

      // Measure GUI summary time
      const guiStart = performance.now();
      const guiSummary = await getGuiSummary({
        screenshot: rawImageBuffer,
        openAiKey: FAKE_OPENAI_KEY,
      });
      const guiEnd = performance.now();
      const guiTimeMs = guiEnd - guiStart;

      // Log results
      console.log(
        \`[\${testCase.complexityLevel.toUpperCase()}] OCR Time: \${(ocrTimeMs / 1000).toFixed(2)}s,\` +
        \` UI AI Time: \${(uiTimeMs / 1000).toFixed(2)}s,\` +
        \` GUI AI Time: \${(guiTimeMs / 1000).toFixed(2)}s\`
      );

      // Basic performance assertions
      expect(ocrTimeMs).toBeLessThan(120000);
      expect(uiTimeMs).toBeLessThan(180000);
      expect(guiTimeMs).toBeLessThan(180000);

      // Verify the AI summaries are not empty
      expect(uiSummary).toBeTruthy();
      expect(guiSummary).toBeTruthy();
    });
  });
});
</code>
  </pre>

  <h3 style="font-size:1.25rem; margin-bottom:0.5rem;">Continuous Integration and Future Expansion</h3>
  <p style="margin-bottom:1rem;">
    All tests run automatically via our continuous integration (CI) pipeline whenever new
    code is pushed to the repository. This guarantees that OCR, AI summarization, and final
    layout rendering remain stable with each commit. Additional tests can be introduced to
    target newly discovered edge cases or extended modules, ensuring that coverage scales
    alongside new features like multi-page generation, more advanced design transformations,
    and refined AI prompts.
  </p>

  <h3 style="font-size:1.25rem; margin-bottom:0.5rem;">Relevance to the Overall Project</h3>
  <p style="margin-bottom:1rem;">
    By thoroughly testing each core function—OCR accuracy, bounding box validations, confidence
    thresholds, AI handling of recognized text, and layout generation logic—we uphold the
    end-to-end reliability of the entire AI-driven workflow. These rigorous checks reduce the
    risk of regressions and user-facing errors, ensuring that both the <strong>First Page Creator</strong>
    and the <strong>Main Interface</strong> consistently deliver the expected user experience.
  </p>
</section>

    </pre>
  </section>

  <!-- Compatibility Testing -->
<section id="compatibility-testing" class="mb-5" style="margin-bottom:2rem;">
  <h2 style="font-size:1.75rem; margin-bottom:1rem;">Compatibility Testing</h2>
  <p style="margin-bottom:1rem;">
    We run exhaustive compatibility checks across Windows, macOS, and Linux. Our tests
    include verifying the VS Code extension’s performance, ensuring the Python OCR scripts
    run consistently, and validating that the webview UI renders and behaves properly in
    embedded frames or native browsers on each platform. This helps guarantee that both
    the AI-driven page creation process and the CraftJS editor flow remain stable and
    visually consistent, regardless of operating system specifics.
  </p>
  <p style="margin-bottom:1rem;">
    Machines with varied CPU/GPU configurations are used to confirm that image processing,
    memory usage, and text extraction remain stable over a wide range of hardware. We ensure
    that users on both resource-constrained setups (e.g., older laptops) and high-performance
    workstations (with advanced graphics capabilities) can run the application without
    encountering crashes or major slowdowns. This involves validating that the Python OCR
    service does not exceed memory limits and that the AI summarization pipeline remains
    responsive, even under heavier loads.
  </p>

  <h3 style="font-size:1.25rem; margin-bottom:0.5rem;">Platform Range</h3>
  <p style="margin-bottom:1rem;">
    Our testing environment covers multiple distributions of Linux (Ubuntu, Fedora),
    multiple macOS versions, and Windows 10/11. On each platform, we install the VS
    Code extension locally and verify:
  </p>
  <ul style="margin-bottom:1rem; list-style-type: disc; margin-left:2rem;">
    <p>- Extension activation, including correct loading of Python-based OCR components.</p>
    <p>- Proper initialization of the webview UI, ensuring React components and styling load without error.</p>
    <p>- Full functionality of AI-driven layout generation, from uploading images to rendering the final CraftJS layout.</p>
  </ul>

  <h3 style="font-size:1.25rem; margin-bottom:0.5rem;">VS Code Extension Integrations</h3>
  <p style="margin-bottom:1rem;">
    We confirm that the extension maintains reliable communication between the front-end
    React interface and the back-end Python OCR services. Cross-version checks of VS Code
    itself are included to ensure that no compatibility issues arise from varying API levels.
  </p>

  <h3 style="font-size:1.25rem; margin-bottom:0.5rem;">GPU/CPU Variations</h3>
  <p style="margin-bottom:1rem;">
    Python OCR (EasyOCR) benefits from GPU acceleration in some cases, so we validate both
    GPU-enabled and CPU-only usage. Test scenarios confirm that text extraction and bounding
    box predictions remain accurate without sacrificing performance, regardless of whether
    the underlying hardware is a high-end GPU or a basic integrated graphics chip.
  </p>
</section>

<!-- Responsive Design Testing -->
<section id="responsive-design-testing" class="mb-5" style="margin-bottom:2rem;">
  <h2 style="font-size:1.75rem; margin-bottom:1rem;">Responsive Design Testing</h2>
  <p style="margin-bottom:1rem;">
    Our application is tested on multiple screen sizes—ranging from high-resolution displays
    to more modest laptops—to ensure that UI elements, sidebars, and the draggable canvas
    remain usable and visually clear at any scale. Touchscreen devices also receive focused
    testing to confirm that drag-and-drop operations function as expected. This is crucial for
    maintaining a smooth user experience across desktops, tablets, and convertible laptops.
  </p>

  <h3 style="font-size:1.25rem; margin-bottom:0.5rem;">Breakpoints and Screen Resolutions</h3>
  <p style="margin-bottom:1rem;">
    We test standard breakpoints (e.g., 1280px, 1024px, 768px, 480px) to confirm that components
    auto-adjust their layouts accordingly. The CraftJS canvas accommodates these breakpoints by
    scaling appropriately, ensuring that no overlapping or hidden interface elements appear at
    smaller resolutions.
  </p>

  <h3 style="font-size:1.25rem; margin-bottom:0.5rem;">Touchscreen and Pointer Interactions</h3>
  <p style="margin-bottom:1rem;">
    We verify that dragging, resizing, and rotating elements in the CraftJS canvas are smooth
    when using touch gestures. Similarly, pointer-based interactions (e.g., trackpads, styluses)
    also undergo tests to confirm consistent behavior. During these tests, we pay particular
    attention to any potential delays or missed events, ensuring that the user experience
    remains intuitive.
  </p>

  <p style="margin-bottom:1rem;">
    We also combine real-world device checks with browser developer tools for testing
    responsiveness. Every major element—such as component sidebars, property panels, and
    advanced editing features—undergoes repeated iteration to confirm fluid adaptation to
    different display widths and orientations.
  </p>
</section>

<!-- Performance/Stress Testing -->
<section id="performance-stress-testing" class="mb-5" style="margin-bottom:2rem;">
  <h2 style="font-size:1.75rem; margin-bottom:1rem;">Performance/Stress Testing</h2>
  <p style="margin-bottom:1rem;">
    To verify scalability and robust handling of real-world scenarios, we subject the application
    to varying image complexities (e.g., “Very Simple” to “Extremely Dense”) and different user
    prompts. Below are visual summaries of OCR confidence, character error rates, AI
    processing times, and overall accuracy. These metrics illustrate how our upscaling/grayscale
    preprocessing and AI summarization adapt to diverse workloads.
  </p>

  <h3 style="font-size:1.25rem; margin-bottom:0.5rem;">OCR Confidence and Accuracy</h3>
  <figure style="margin-bottom:2rem;">
    <img 
      src="./testing-assets/OCR_CONFIDENCE.png" 
      alt="OCR Confidence Scores" 
      style="max-width: 60%; border:1px solid #ccc;"
    />
    <figcaption style="margin-top:0.5rem; font-style:italic;">
      Average OCR Confidence Scores by Preprocessing Method
    </figcaption>
  </figure>
  <p style="margin-bottom:1rem;">
    The <strong>Upscale + Grayscale</strong> method demonstrates the highest confidence levels
    (ranging from about 0.88 to 0.95 across different test images). <strong>Grayscale Only</strong>
    remains moderately strong but can dip under 0.8 for complex images, while
    <strong>No Preprocessing</strong> often produces the lowest confidence (around 0.55 to 0.65)
    for items like MobileUI or ProductLabel that contain small or stylized text.
  </p>

  <figure style="margin-bottom:2rem;">
    <img 
      src="./testing-assets/OCR_PREPROCESSING.png" 
      alt="OCR Accuracy by Preprocessing Method" 
      style="max-width: 60%; border:1px solid #ccc;"
    />
    <figcaption style="margin-top:0.5rem; font-style:italic;">
      Comparative OCR Accuracy for different preprocessing options
    </figcaption>
  </figure>
  <p style="margin-bottom:1rem;">
    Similarly, the accuracy chart underscores that <strong>Upscale + Grayscale</strong> yields
    the most consistent text recognition results, especially for screenshots containing
    multiple form fields (e.g., <em>BillingForm</em>) and heavily segmented layouts
    (<em>MobileUI</em>). <strong>No Preprocessing</strong> trails behind significantly, indicating
    that some image adjustments are vital for better OCR performance.
  </p>

  <h3 style="font-size:1.25rem; margin-bottom:0.5rem;">Character Error Rates (CER)</h3>
  <figure style="margin-bottom:2rem;">
    <img 
      src="./testing-assets/CHARACTER_PREPROCESSING.png" 
      alt="Character Error Rate by Preprocessing Method" 
      style="max-width: 60%; border:1px solid #ccc;"
    />
    <figcaption style="margin-top:0.5rem; font-style:italic;">
      Character Error Rate (CER) across different preprocessing approaches
    </figcaption>
  </figure>
  <p style="margin-bottom:1rem;">
    Character error rate measurements illustrate where certain methods excel. 
    <strong>Upscale + Grayscale</strong> consistently keeps CER below 10%, even for complex text
    segments, while <strong>No Preprocessing</strong> can climb above 30% in some cases. 
    This gap is especially evident when analyzing small receipt text or intricate UI labels.
  </p>

  <h3 style="font-size:1.25rem; margin-bottom:0.5rem;">AI Processing Time vs Image Complexity</h3>
  <figure style="margin-bottom:2rem;">
    <img 
      src="./testing-assets/PROCESSING_COMPLEXITY.png" 
      alt="AI Processing Time vs Image Complexity" 
      style="max-width: 60%; border:1px solid #ccc;"
    />
    <figcaption style="margin-top:0.5rem; font-style:italic;">
      AI Processing Time (OCR and CraftJS output) for varied complexity levels
    </figcaption>
  </figure>
  <p style="margin-bottom:1rem;">
    As image complexity increases, both OCR and CraftJS output times rise proportionally.
    Simple images (e.g., a basic login page) complete OCR in approximately 19 seconds and
    CraftJS output in around 18 seconds, while extremely dense images (e.g., 
    large multi-section forms or lengthy product labels) can push OCR to about 230 seconds
    and CraftJS output to 343 seconds. This predictable scaling allows us to benchmark
    improvements in the future and optimize our AI pipeline for increasingly dense screenshots.
  </p>

  <h3 style="font-size:1.25rem; margin-bottom:0.5rem;">Overall Observations</h3>
  <p style="margin-bottom:1rem;">
    Our stress tests confirm that specialized preprocessing (upscaling, grayscaling, and
    careful cleanup) significantly boosts OCR reliability. This, in turn, feeds more accurate
    textual data into the AI summarization layers, ensuring that the final CraftJS layouts
    reflect the correct information. Meanwhile, the increased processing time for extremely
    dense images remains a key area for future optimization, particularly in concurrency or
    real-time user feedback scenarios.
  </p>
  <p style="margin-bottom:1rem;">
    In summary, from straightforward login pages to complex multi-section invoices, the system
    demonstrates strong resilience and accuracy. Our combination of advanced image preprocessing,
    structured AI summarization prompts, and performance-oriented architecture enables rapid,
    high-quality page generation under a variety of test conditions.
  </p>
</section>



    <section id="user-acceptance-testing" class="mb-5">
      <h2>User Acceptance Testing (UAT)</h2>    
      <div>
        <p>User Acceptance Testing (UAT) was conducted to ensure that BlueprintAI meets functional requirements and delivers a smooth experience for developers and designers. Testing focused on usability, performance, and the effectiveness of AI-generated UI components.</p>
      </div>
    
      <div class="testers">
        <h3>Testers</h3>
        <ul>
          <strong>Tester 1</strong> – 22-year-old Computer Science student<br>
          <strong>Tester 2</strong> – 28-year-old UX Designer<br>
          <strong>Tester 3</strong> – 35-year-old Frontend Developer<br>
          <strong>Tester 4</strong> – 41-year-old Project Manager<br>
        </ul>
        <p>These testers were selected to represent our key user groups, including students, designers, developers, and project managers. Their identities have been anonymised for privacy.</p>
      </div>
      
      <div class="test-cases">
        <h3>Test Cases</h3>
        <ol>
          <strong>Test Case 1: Screenshot-to-Layout Generation</strong><br>
            We asked the user to upload a screenshot of a webpage and generate a structured layout using AI.<br>
          <strong>Test Case 2: Text-to-Layout Generation</strong><br>
            We asked the user to describe a webpage in natural language and observe how well the AI-generated layout matched their description.<br>
          <strong>Test Case 3: Drag-and-Drop Customisation</strong><br>
            Users were instructed to edit and arrange UI components using the drag-and-drop editor.<br>
          <strong>Test Case 4: AI-Driven Component Suggestions</strong><br>
            Users selected a UI element and requested AI-generated design suggestions for enhancements.<br>
          <strong>Test Case 5: Export Functionality (HTML, CSS, JavaScript)</strong><br>
            Users were asked to export their projects as HTML, CSS, and JavaScript files and verify the quality of the generated code.<br>
        </ol>
      </div>
      

    <div class="table-responsive">
      <h3>Feedback from Users</h3>
      <table class="table table-bordered table-hover">
        <thead class="table-light">
          <tr>
            <th>Acceptance Requirement</th>
            <th>Strongly Disagree</th>
            <th>Disagree</th>
            <th>Agree</th>
            <th>Strongly Agree</th>
            <th>Comments</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Was the UI navigation easy?</td>
            <td>0</td>
            <td>0</td>
            <td>1</td>
            <td>3</td>
            <td>+ Clean and simple interface</td>
          </tr>
          <tr>
            <td>Was the AI-generated layout accurate?</td>
            <td>0</td>
            <td>1</td>
            <td>2</td>
            <td>1</td>
            <td>- Struggled with complex layouts, but overall useful.</td>
          </tr>
          <tr>
            <td>Could you easily drag and drop components?</td>
            <td>0</td>
            <td>0</td>
            <td>2</td>
            <td>2</td>
            <td>+ Alignment guides helped improve usability</td>
          </tr>
          <tr>
            <td>Were AI design suggestions helpful?</td>
            <td>0</td>
            <td>1</td>
            <td>2</td>
            <td>1</td>
            <td>- Suggestions were useful, but customization options could be expanded</td>
          </tr>
          <tr>
            <td>Could you successfully export your project?</td>
            <td>0</td>
            <td>0</td>
            <td>1</td>
            <td>3</td>
            <td>+ Exported code was well-structured and easy to use in VSCode.</td>
          </tr>
          <tr>
            <td>Was the AI-generated text-to-layout accurate?</td>
            <td>0</td>
            <td>1</td>
            <td>2</td>
            <td>1</td>
            <td>- Simple layouts worked well, but needed refinement for complex designs.</td>
          </tr>
          <tr>
            <td>Was the experience smooth overall?</td>
            <td>0</td>
            <td>0</td>
            <td>1</td>
            <td>3</td>
            <td>+ Intuitive, but some minor bugs when handling large projects</td>
          </tr>
        </tbody>
      </table>
    </div>
    
      <div>
        <p>Based on the above criteria, we also asked a broader group of 15 peers to test out our product. The 4 main criteria produced the below results. </p>
      <div class="row mt-4">
        <h3>Peer Feedback</h3>
        <img src="testing-assets/UAT_charts.png" alt="UAT Charts">
      </div>
      <div>
        <h3>Conclusion</h3>
        <p>The user feedback was overwhelmingly positive, indicating that BlueprintAI successfully provides an accessible and efficient AI-powered UI design experience. Users appreciated the drag-and-drop customisation, AI-generated layouts, and export functionality while also identifying areas for improvement, such as better handling of complex designs and more customisation options for AI suggestions.</p>
      </div>
    </section>
    <div class="floating-buttons">
      <button id="topSection" class="section-nav-btn" title="Back to Top">Back To Top</button>
    </div>
    
  </main>

  <!-- Footer -->
  <footer class="text-center py-4 mt-auto">
    <div class="footer-logos d-flex justify-content-center align-items-center">
      <a href="https://www.ucl.ac.uk" target="_blank">
        <img src="ucl.png" alt="UCL Logo" class="footer-logo">
      </a>
      <a href="https://www.microsoft.com" target="_blank">
        <img src="microsoft.png" alt="Microsoft Logo" class="footer-logo">
      </a>
      <a href="https://github.com/YOUR_GITHUB" target="_blank">
        <i class="fa-brands fa-square-github"></i>
      </a>
    </div>
  </footer>

  <!-- Bootstrap JS Bundle (includes Popper) -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      const topBtn = document.getElementById('topSection');
  
      topBtn?.addEventListener('click', () => {
        window.scrollTo({ top: 0, behavior: 'smooth' });
      });
    });
  </script>
  
  <!-- Optional custom scripts -->
  <script src="script.js"></script>
</body>
</html>
